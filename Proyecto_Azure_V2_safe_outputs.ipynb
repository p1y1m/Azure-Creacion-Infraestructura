{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNokVzQQ+2lQGyw0NwyKZRp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRdFgvIp95V0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758632349248,
          "user_tz": 180,
          "elapsed": 45244,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "b586c18b-ded0-4173-db52-e987a820cfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,646 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,576 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,807 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,299 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,327 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Fetched 22.3 MB in 8s (2,696 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'removed-for-safety' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lsb-release is already the newest version (11.1.0ubuntu4).\n",
            "lsb-release set to manually installed.\n",
            "ca-certificates is already the newest version (20240203~22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.4).\n",
            "gnupg set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  apt-transport-https\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 1,510 B of archives.\n",
            "After this operation, 170 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 apt-transport-https all 2.4.14 [1,510 B]\n",
            "Fetched 1,510 B in 0s (6,386 B/s)\n",
            "Selecting previously unselected package apt-transport-https.\n",
            "(Reading database ... 126435 files and directories currently installed.)\n",
            "Preparing to unpack .../apt-transport-https_2.4.14_all.deb ...\n",
            "Unpacking apt-transport-https (2.4.14) ...\n",
            "Setting up apt-transport-https (2.4.14) ...\n",
            "Types: deb\n",
            "URIs: https://packages.microsoft.com/repos/azure-cli/\n",
            "Suites: jammy\n",
            "Components: main\n",
            "Architectures: amd64\n",
            "Signed-by: /etc/apt/keyrings/microsoft.gpg\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:6 https://packages.microsoft.com/repos/azure-cli jammy InRelease [3,596 B]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://packages.microsoft.com/repos/azure-cli jammy/main amd64 Packages [2,640 B]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://packages.microsoft.com/repos/azure-cli jammy/main all Packages [1,093 B]\n",
            "Fetched 11.2 kB in 1s (7,985 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'removed-for-safety' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  azure-cli\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 53.8 MB of archives.\n",
            "After this operation, 597 MB of additional disk space will be used.\n",
            "Get:1 https://packages.microsoft.com/repos/azure-cli jammy/main amd64 azure-cli amd64 2.77.0-1~jammy [53.8 MB]\n",
            "Fetched 53.8 MB in 1s (69.3 MB/s)\n",
            "Selecting previously unselected package azure-cli.\n",
            "(Reading database ... 126439 files and directories currently installed.)\n",
            "Preparing to unpack .../azure-cli_2.77.0-1~jammy_amd64.deb ...\n",
            "Unpacking azure-cli (2.77.0-1~jammy) ...\n",
            "Setting up azure-cli (2.77.0-1~jammy) ...\n"
          ]
        }
      ],
      "source": [
        "# Instalar Azure CLI\n",
        "!curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Terraform\n",
        "!wget https://releases.hashicorp.com/terraform/1.9.5/terraform_1.9.5_linux_amd64.zip\n",
        "!unzip terraform_1.9.5_linux_amd64.zip\n",
        "!sudo mv terraform /usr/local/bin/\n",
        "!terraform -v\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCt32mtV-CyN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758542643484,
          "user_tz": 180,
          "elapsed": 2031,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "1e9d91ae-0402-4365-a40d-81b759e0281d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 11:02:41--  https://releases.hashicorp.com/terraform/1.9.5/terraform_1.9.5_linux_amd64.zip\n",
            "Resolving releases.hashicorp.com (releases.hashicorp.com)... 18.160.225.45, 18.160.225.53, 18.160.225.35, ...\n",
            "Connecting to releases.hashicorp.com (releases.hashicorp.com)|18.160.225.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27040662 (26M) [application/zip]\n",
            "Saving to: ‘terraform_1.9.5_linux_amd64.zip’\n",
            "\n",
            "terraform_1.9.5_lin 100%[===================>]  25.79M   131MB/s    in 0.2s    \n",
            "\n",
            "2025-09-22 11:02:41 (131 MB/s) - ‘terraform_1.9.5_linux_amd64.zip’ saved [27040662/27040662]\n",
            "\n",
            "Archive:  terraform_1.9.5_linux_amd64.zip\n",
            "  inflating: LICENSE.txt             \n",
            "  inflating: terraform               \n",
            "Terraform v1.9.5\n",
            "on linux_amd64\n",
            "\n",
            "Your version of Terraform is out of date! The latest version\n",
            "is 1.13.3. You can update by downloading from https://www.terraform.io/downloads.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXUjS_8w-Ndd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758542860207,
          "user_tz": 180,
          "elapsed": 8532,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "ad0294cb-3570-49bf-a9ec-0b58ba50bc33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ir a la carpeta Proyecto Azure\n",
        "%cd /content/drive/MyDrive/Proyecto\\ Azure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H06_CPpDCq9W",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758632349283,
          "user_tz": 180,
          "elapsed": 23,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "6d828a74-e2f4-483d-c6aa-0f49bbe1b281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'removed-for-safety'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacer login hacia Azure CLI # Presionar 1, y luego enter cuando consulta la subscripción\n",
        "\n",
        "!az login --use-device-code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4O2LirM1BcF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758548021334,
          "user_tz": 180,
          "elapsed": 106990,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "f30822cf-9592-47fc-fd76-c9668021c378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code ES2JU5H4P to authenticate.\u001b[0m\n",
            "\n",
            "Retrieving tenant=removed-for-safety and subscription=removed-for-safety for the selection...\n",
            "\n",
            "[Tenant=removed-for-safety and subscription=removed-for-safety selection]\n",
            "\n",
            "No     Subscription=removed-for-safety name     Subscription=removed-for-safety ID                       Tenant=removed-for-safety\n",
            "-----  removed-for-safety  removed-for-safety  removed-for-safety\n",
            "\u001b[96m[1]\u001b[0m *  \u001b[96mAzure subscription=removed-for-safety 1\u001b[0m  \u001b[96mremoved-for-safety\u001b[0m  \u001b[96mDirectorio predeterminado\u001b[0m\n",
            "\n",
            "The default is marked with an *; the default tenant=removed-for-safety is 'removed-for-safety' and subscription=removed-for-safety is 'removed-for-safety' (removed-for-safety).\n",
            "\n",
            "Select a subscription=removed-for-safety and tenant=removed-for-safety (Type a number or Enter for no changes): 1\n",
            "\n",
            "Tenant=removed-for-safety Directorio predeterminado\n",
            "Subscription=removed-for-safety Azure subscription=removed-for-safety 1 (removed-for-safety)\n",
            "\n",
            "[Announcements]\n",
            "With the new Azure CLI login experience, you can select the subscription=removed-for-safety you want to use more easily. Learn more about it and its configuration at https://go.microsoft.com/fwlink/?linkid=2271236\n",
            "\n",
            "If you encounter any problem, please open an issue at https://aka.ms/azclibug\n",
            "\n",
            "\u001b[93m[Warning] The login output has been updated. Please be aware that it no longer displays the full list of available subscription=removed-for-safety by default.\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NO VOLVER A REALIZAR, SOLO UNA VEZ SE DEBIÓ CREAR EL ROL\n",
        "\n",
        "#!az role assignment create \\\n",
        "  --assignee 73e02e62-da0c-448f-9d47-88debddd5788 \\\n",
        "  --role \"Contributor\" \\\n",
        "  --scope /subscriptions/56060326-71b8-4fac-a5e2-2d3ead729a62\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1i7nmQd2Fxi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758266426316,
          "user_tz": 180,
          "elapsed": 5634,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "0b301500-2e34-4949-c6a2-28c0b651692e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"condition\": null,\n",
            "  \"conditionVersion\": null,\n",
            "  \"createdBy\": \"removed-for-safety\",\n",
            "  \"createdOn\": \"removed-for-safety\",\n",
            "  \"removed-for-safety\": null,\n",
            "  \"description\": null,\n",
            "  \"id\": \"removed-for-safety\",\n",
            "  \"name\": \"removed-for-safety\",\n",
            "  \"principalId\": \"removed-for-safety\",\n",
            "  \"principalName\": \"removed-for-safety\",\n",
            "  \"principalType\": \"ServicePrincipal\",\n",
            "  \"roleDefinitionId\": \"removed-for-safety\",\n",
            "  \"roleDefinitionName\": \"Contributor\",\n",
            "  \"scope\": \"removed-for-safety\",\n",
            "  \"type\": \"removed-for-safety\",\n",
            "  \"updatedBy\": \"removed-for-safety\",\n",
            "  \"updatedOn\": \"removed-for-safety\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x .terraform/providers/registry.terraform.io/hashicorp/azurerm/4.45.0/linux_amd64/terraform-provider-azurerm_v4.45.0_x5"
      ],
      "metadata": {
        "id": "WG7uvCn0ZY5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar terraform y ver el plan # Más adelante borrar estas credenciales\n",
        "\n",
        "!export ARM_CLIENT_ID=\"removed-for-safety\" && \\\n",
        "export ARM_CLIENT_SECRET=\"removed-for-safety\" && \\\n",
        "export ARM_SUBSCRIPTION_ID=\"removed-for-safety\" && \\\n",
        "export ARM_TENANT_ID=\"removed-for-safety\" && \\\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIFCrbw94FWj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758548123792,
          "user_tz": 180,
          "elapsed": 13970,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "bb02249e-43b7-426e-a193-2ce7915ec0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[1mInitializing the backend...\u001b[0m\n",
            "\u001b[0m\u001b[1mInitializing provider plugins...\u001b[0m\n",
            "- Reusing previous version of hashicorp/azurerm from the dependency lock file\n",
            "- Using previously-installed hashicorp/azurerm v4.45.0\n",
            "\n",
            "\u001b[0m\u001b[1m\u001b[32mTerraform has been successfully initialized!\u001b[0m\u001b[32m\u001b[0m\n",
            "\u001b[0m\u001b[32m\n",
            "You may now begin working with Terraform. Try running \"terraform plan\" to see\n",
            "any changes that are required for your infrastructure. All Terraform commands\n",
            "should now work.\n",
            "\n",
            "If you ever set or change modules or backend configuration for Terraform,\n",
            "rerun this command to reinitialize your working directory. If you forget, other\n",
            "commands will detect it and remind you to do so if necessary.\u001b[0m\n",
            "\n",
            "Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n",
            "  \u001b[32m+\u001b[0m create\u001b[0m\n",
            "\n",
            "Terraform will perform the following actions:\n",
            "\n",
            "\u001b[1m  # azurerm_data_factory.df\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_data_factory\" \"df\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m customer_managed_key_id          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                         = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                             = \"dfpipeline123\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_enabled           = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name              = \"rg-pipeline-demo\"\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_database.sqldb\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_database\" \"sqldb\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m auto_pause_delay_in_minutes                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m collation                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m create_mode                                                = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m creation_source_database_id                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m enclave_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m geo_backup_enabled                                         = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m ledger_enabled                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m license_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m max_size_gb                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m min_capacity                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                                       = \"sqldbpipeline\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_replica_count                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_scale                                                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m restore_point_in_time                                      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sample_name                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_type                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m server_id                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sku_name                                                   = \"S0\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m storage_account_type                                       = \"Geo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                        = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m zone_redundant                                             = (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m long_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m short_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m threat_detection_policy (known after apply)\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_server.sqlserver\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_server\" \"sqlserver\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login                      = \"sqladminuser\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login_password=removed-for-safety             = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m connection_policy                        = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m fully_qualified_domain_name              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                                 = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m minimum_tls_version                      = \"1.2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                     = \"sqlserverpipeline123\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_access_enabled            = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name                      = \"rg-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m version                                  = \"12.0\"\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_resource_group.rg\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_resource_group\" \"rg\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name     = \"rg-pipeline-demo\"\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_storage_account.st\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_storage_account\" \"st\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m access_tier                        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m account_kind                       = \"StorageV2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m account_replication_type           = \"LRS\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m account_tier                       = \"Standard\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m cross_tenant=removed-for-safety   = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m dns_endpoint_type                  = \"Standard\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m https_traffic_only_enabled         = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety  = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m is_hns_enabled                     = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m large_file_share_enabled           = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m local_user_enabled                 = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                           = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m min_tls_version                    = \"TLS1_2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                               = \"storagepipeline123\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m nfsv3_enabled                      = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_access_key                 = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_endpoint              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_host                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_internet_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_microsoft_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_connection_string          = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_endpoint               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_host                   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_internet_endpoint      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_internet_host          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_microsoft_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_endpoint              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_host                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_internet_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_microsoft_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_location                   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_queue_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_queue_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_queue_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_table_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_table_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_table_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_endpoint               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_host                   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_internet_endpoint      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_internet_host          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_microsoft_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_access_enabled      = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m queue_encryption_key_type          = \"Service\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name                = \"rg-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_access_key               = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_endpoint            = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_host                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_internet_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_microsoft_host      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_connection_string        = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_internet_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_endpoint            = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_host                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_internet_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_microsoft_host      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_location                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_queue_endpoint           = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_queue_host               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_table_endpoint           = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_table_host               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_internet_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sftp_enabled                       = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m shared_access_key_enabled          = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m table_encryption_key_type          = \"Service\"\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m blob_properties (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m network_rules (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m queue_properties (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m routing (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m share_properties (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m static_website (known after apply)\n",
            "    }\n",
            "\n",
            "\u001b[1mPlan:\u001b[0m 5 to add, 0 to change, 0 to destroy.\n",
            "\u001b[0m\n",
            "\n",
            "\n",
            "Saved the plan to: tfplan\n",
            "\n",
            "To perform exactly these actions, run the following command to apply:\n",
            "    terraform apply \"tfplan\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Inicializar Terraform (descarga providers y prepara el entorno)\n",
        "!terraform init\n",
        "\n",
        "# 2. Generar el plan y guardarlo en un archivo\n",
        "!terraform plan -out=tfplan\n",
        "\n",
        "# 3. Aplicar el plan guardado (crea los recursos en Azure)\n",
        "!terraform apply -auto-approve tfplan\n",
        "\n",
        "# Se crearon los recursos excepto base de datos SQL, por lo cual se modificó el main.tf, cambiando la location de base de datos sql\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyDoRlpEb5H8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758549369639,
          "user_tz": 180,
          "elapsed": 99920,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "3473f702-d13a-49a7-d7db-306a495fb966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[1mInitializing the backend...\u001b[0m\n",
            "\u001b[0m\u001b[1mInitializing provider plugins...\u001b[0m\n",
            "- Reusing previous version of hashicorp/azurerm from the dependency lock file\n",
            "- Using previously-installed hashicorp/azurerm v4.45.0\n",
            "\n",
            "\u001b[0m\u001b[1m\u001b[32mTerraform has been successfully initialized!\u001b[0m\u001b[32m\u001b[0m\n",
            "\u001b[0m\u001b[32m\n",
            "You may now begin working with Terraform. Try running \"terraform plan\" to see\n",
            "any changes that are required for your infrastructure. All Terraform commands\n",
            "should now work.\n",
            "\n",
            "If you ever set or change modules or backend configuration for Terraform,\n",
            "rerun this command to reinitialize your working directory. If you forget, other\n",
            "commands will detect it and remind you to do so if necessary.\u001b[0m\n",
            "\n",
            "Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n",
            "  \u001b[32m+\u001b[0m create\u001b[0m\n",
            "\n",
            "Terraform will perform the following actions:\n",
            "\n",
            "\u001b[1m  # azurerm_data_factory.df\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_data_factory\" \"df\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m customer_managed_key_id          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                         = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                             = \"dfpipeline123\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_enabled           = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name              = \"rg-pipeline-demo\"\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_database.sqldb\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_database\" \"sqldb\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m auto_pause_delay_in_minutes                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m collation                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m create_mode                                                = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m creation_source_database_id                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m enclave_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m geo_backup_enabled                                         = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m ledger_enabled                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m license_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m max_size_gb                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m min_capacity                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                                       = \"sqldbpipeline\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_replica_count                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_scale                                                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m restore_point_in_time                                      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sample_name                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_type                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m server_id                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sku_name                                                   = \"S0\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m storage_account_type                                       = \"Geo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                        = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m zone_redundant                                             = (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m long_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m short_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m threat_detection_policy (known after apply)\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_server.sqlserver\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_server\" \"sqlserver\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login                      = \"sqladminuser\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login_password=removed-for-safety             = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m connection_policy                        = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m fully_qualified_domain_name              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                                 = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m minimum_tls_version                      = \"1.2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                     = \"sqlserverpipeline123\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_access_enabled            = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name                      = \"rg-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m version                                  = \"12.0\"\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_resource_group.rg\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_resource_group\" \"rg\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name     = \"rg-pipeline-demo\"\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_storage_account.st\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_storage_account\" \"st\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m access_tier                        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m account_kind                       = \"StorageV2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m account_replication_type           = \"LRS\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m account_tier                       = \"Standard\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m cross_tenant=removed-for-safety   = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m dns_endpoint_type                  = \"Standard\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m https_traffic_only_enabled         = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety  = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m is_hns_enabled                     = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m large_file_share_enabled           = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m local_user_enabled                 = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                           = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m min_tls_version                    = \"TLS1_2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                               = \"storagepipeline123\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m nfsv3_enabled                      = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_access_key                 = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_endpoint              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_host                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_internet_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_blob_microsoft_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_connection_string          = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_endpoint               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_host                   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_internet_endpoint      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_internet_host          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_dfs_microsoft_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_endpoint              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_host                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_internet_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_file_microsoft_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_location                   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_queue_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_queue_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_queue_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_table_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_table_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_table_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_endpoint               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_host                   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_internet_endpoint      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_internet_host          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m primary_web_microsoft_host         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_access_enabled      = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m queue_encryption_key_type          = \"Service\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name                = \"rg-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_access_key               = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_endpoint            = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_host                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_internet_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_blob_microsoft_host      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_connection_string        = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_internet_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_dfs_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_endpoint            = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_host                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_internet_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_file_microsoft_host      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_location                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_queue_endpoint           = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_queue_host               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_table_endpoint           = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_table_host               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_endpoint             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_host                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety    = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_internet_host        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety   = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_web_microsoft_host       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sftp_enabled                       = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m shared_access_key_enabled          = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m table_encryption_key_type          = \"Service\"\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m blob_properties (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m network_rules (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m queue_properties (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m routing (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m share_properties (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m static_website (known after apply)\n",
            "    }\n",
            "\n",
            "\u001b[1mPlan:\u001b[0m 5 to add, 0 to change, 0 to destroy.\n",
            "\u001b[0m\n",
            "\n",
            "\n",
            "Saved the plan to: tfplan\n",
            "\n",
            "To perform exactly these actions, run the following command to apply:\n",
            "    terraform apply \"tfplan\"\n",
            "\u001b[0m\u001b[1mazurerm_resource_group.rg: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_resource_group.rg: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_resource_group.rg: Creation complete after 11s [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_data_factory.df: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_data_factory.df: Creation complete after 5s [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Still creating... [40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Still creating... [50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Still creating... [1m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Creation complete after 1m5s [id=/subscription=removed-for-safety\n",
            "\u001b[31m╷\u001b[0m\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\u001b[1m\u001b[31mError: \u001b[0m\u001b[0m\u001b[1mcreating Server (Subscription=removed-for-safety \"removed-for-safety\"\n",
            "\u001b[31m│\u001b[0m \u001b[0mResource Group Name: \"rg-pipeline-demo\"\n",
            "\u001b[31m│\u001b[0m \u001b[0mServer Name: \"sqlserverpipeline123\"): polling after CreateOrUpdate: polling failed: the Azure API returned the following error:\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0mStatus: \"ProvisioningDisabled\"\n",
            "\u001b[31m│\u001b[0m \u001b[0mCode: \"\"\n",
            "\u001b[31m│\u001b[0m \u001b[0mMessage: \"removed-for-safety'Service and subscription=removed-for-safety limits'removed-for-safety\"\n",
            "\u001b[31m│\u001b[0m \u001b[0mActivity Id: \"\"\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m---\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0mAPI Response:\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m----[start]----\n",
            "\u001b[31m│\u001b[0m \u001b[0m{\"name\":\"removed-for-safety\",\"status\":\"Failed\",\"startTime\":\"2025-09-22T12:53:45.297Z\",\"error\":{\"code\":\"ProvisioningDisabled\",\"message\":\"removed-for-safety'Service and subscription=removed-for-safety limits'removed-for-safety\"}}\n",
            "\u001b[31m│\u001b[0m \u001b[0m-----[end]-----\n",
            "\u001b[31m│\u001b[0m \u001b[0m\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\u001b[0m  with azurerm_mssql_server.sqlserver,\n",
            "\u001b[31m│\u001b[0m \u001b[0m  on main.tf line 35, in resource \"azurerm_mssql_server\" \"sqlserver\":\n",
            "\u001b[31m│\u001b[0m \u001b[0m  35: resource \"azurerm_mssql_server\" \"sqlserver\" \u001b[4m{\u001b[0m\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m╵\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ejecutará nuevamente , dado que se modificó la location del recurso sql en main.tf\n",
        "\n",
        "!terraform plan -out=tfplan\n",
        "!terraform apply -auto-approve tfplan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr0uSywsxIEE",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758550511380,
          "user_tz": 180,
          "elapsed": 26509,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "40b9ef36-373c-4c05-a453-04a8935bc8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[1mazurerm_resource_group.rg: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_data_factory.df: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\n",
            "Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n",
            "  \u001b[32m+\u001b[0m create\u001b[0m\n",
            "\n",
            "Terraform will perform the following actions:\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_database.sqldb\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_database\" \"sqldb\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m auto_pause_delay_in_minutes                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m collation                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m create_mode                                                = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m creation_source_database_id                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m enclave_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m geo_backup_enabled                                         = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m ledger_enabled                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m license_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m max_size_gb                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m min_capacity                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                                       = \"sqldbpipeline\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_replica_count                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_scale                                                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m restore_point_in_time                                      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sample_name                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_type                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m server_id                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sku_name                                                   = \"S0\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m storage_account_type                                       = \"Geo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                        = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m zone_redundant                                             = (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m long_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m short_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m threat_detection_policy (known after apply)\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_server.sqlserver\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_server\" \"sqlserver\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login                      = \"sqladminuser\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login_password=removed-for-safety             = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m connection_policy                        = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m fully_qualified_domain_name              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                                 = \"eastus2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m minimum_tls_version                      = \"1.2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                     = \"sqlserverpipeline123\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_access_enabled            = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name                      = \"rg-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m version                                  = \"12.0\"\n",
            "    }\n",
            "\n",
            "\u001b[1mPlan:\u001b[0m 2 to add, 0 to change, 0 to destroy.\n",
            "\u001b[0m\n",
            "\n",
            "\n",
            "Saved the plan to: tfplan\n",
            "\n",
            "To perform exactly these actions, run the following command to apply:\n",
            "    terraform apply \"tfplan\"\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[31m╷\u001b[0m\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\u001b[1m\u001b[31mError: \u001b[0m\u001b[0m\u001b[1mcreating Server (Subscription=removed-for-safety \"removed-for-safety\"\n",
            "\u001b[31m│\u001b[0m \u001b[0mResource Group Name: \"rg-pipeline-demo\"\n",
            "\u001b[31m│\u001b[0m \u001b[0mServer Name: \"sqlserverpipeline123\"removed-for-safety'sqlserverpipeline123'removed-for-safety'eastus' in resource group 'rg-pipeline-demo'removed-for-safety'eastus2'. Please select a new resource name.\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\u001b[0m  with azurerm_mssql_server.sqlserver,\n",
            "\u001b[31m│\u001b[0m \u001b[0m  on main.tf line 35, in resource \"azurerm_mssql_server\" \"sqlserver\":\n",
            "\u001b[31m│\u001b[0m \u001b[0m  35: resource \"azurerm_mssql_server\" \"sqlserver\" \u001b[4m{\u001b[0m\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[0m\n",
            "\u001b[31m╵\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuevamente se realiza cambio en main.tf para poder agregar el recurso SQL\n",
        "\n",
        "!terraform plan -out=tfplan\n",
        "!terraform apply -auto-approve tfplan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3F2eANu1Get",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758551653397,
          "user_tz": 180,
          "elapsed": 236220,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "e3260028-7b4c-4344-8c17-52d6af6350df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[1mazurerm_resource_group.rg: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_data_factory.df: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\n",
            "Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n",
            "  \u001b[32m+\u001b[0m create\u001b[0m\n",
            "\n",
            "Terraform will perform the following actions:\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_database.sqldb\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_database\" \"sqldb\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m auto_pause_delay_in_minutes                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m collation                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m create_mode                                                = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m creation_source_database_id                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m enclave_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m geo_backup_enabled                                         = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m ledger_enabled                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m license_type                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m max_size_gb                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m min_capacity                                               = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                                       = \"sqldbpipeline\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_replica_count                                         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m read_scale                                                 = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m restore_point_in_time                                      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sample_name                                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m secondary_type                                             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m server_id                                                  = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sku_name                                                   = \"S0\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m storage_account_type                                       = \"Geo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety                        = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m zone_redundant                                             = (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m long_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m short_term_retention_policy (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m threat_detection_policy (known after apply)\n",
            "    }\n",
            "\n",
            "\u001b[1m  # azurerm_mssql_server.sqlserver\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_mssql_server\" \"sqlserver\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login                      = \"sqladminuser\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m administrator_login_password=removed-for-safety             = (sensitive value)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m connection_policy                        = \"Default\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m fully_qualified_domain_name              = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                       = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                                 = \"eastus2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m minimum_tls_version                      = \"1.2\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                                     = \"removed-for-safety\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety     = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety        = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_access_enabled            = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name                      = \"rg-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m version                                  = \"12.0\"\n",
            "    }\n",
            "\n",
            "\u001b[1mPlan:\u001b[0m 2 to add, 0 to change, 0 to destroy.\n",
            "\u001b[0m\n",
            "\n",
            "\n",
            "Saved the plan to: tfplan\n",
            "\n",
            "To perform exactly these actions, run the following command to apply:\n",
            "    terraform apply \"tfplan\"\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [1m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [1m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Still creating... [1m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Creation complete after 1m22s [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [1m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [1m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [1m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [1m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [1m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [1m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Still creating... [2m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Creation complete after 2m8s [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "Apply complete! Resources: 2 added, 0 changed, 0 destroyed.\n",
            "\u001b[0m\u001b[0m\u001b[1m\u001b[32m\n",
            "Outputs:\n",
            "\n",
            "\u001b[0mdata_factory_name = \"dfpipeline123\"\n",
            "resource_group_name = \"rg-pipeline-demo\"\n",
            "sql_database_name = \"sqldbpipeline\"\n",
            "storage_account_name = \"storagepipeline123\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listado de recursos creados\n",
        "!az resource list --resource-group rg-pipeline-demo -o table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jasCOuVM6YFi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758552790387,
          "user_tz": 180,
          "elapsed": 18182,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "8d24933f-98e8-450b-9b3e-49750a900af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name                                       ResourceGroup     Location    Type                               Status\n",
            "removed-for-safety  ----------------  ----------  removed-for-safety  --------\n",
            "dfpipeline123                              rg-pipeline-demo  eastus      Microsoft.DataFactory/factories\n",
            "storagepipeline123                         rg-pipeline-demo  eastus      Microsoft.Storage/storageAccounts\n",
            "sqlserverpipeline123eastus2                rg-pipeline-demo  eastus2     Microsoft.Sql/servers\n",
            "sqlserverpipeline123eastus2/sqldbpipeline  rg-pipeline-demo  eastus2     Microsoft.Sql/servers/databases\n",
            "sqlserverpipeline123eastus2/master         rg-pipeline-demo  eastus2     Microsoft.Sql/servers/databases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora se creará el recurso Data Bricks"
      ],
      "metadata": {
        "id": "oNuhYoswApZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!terraform init\n",
        "!terraform plan -out=tfplan\n",
        "!terraform apply -auto-approve tfplan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wOGuI02Czaq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758555733107,
          "user_tz": 180,
          "elapsed": 762198,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "45fc79fc-7753-40f8-c09b-6848f9824c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[1mInitializing the backend...\u001b[0m\n",
            "\u001b[0m\u001b[1mInitializing provider plugins...\u001b[0m\n",
            "- Reusing previous version of hashicorp/azurerm from the dependency lock file\n",
            "- Using previously-installed hashicorp/azurerm v4.45.0\n",
            "\n",
            "\u001b[0m\u001b[1m\u001b[32mTerraform has been successfully initialized!\u001b[0m\u001b[32m\u001b[0m\n",
            "\u001b[0m\u001b[32m\n",
            "You may now begin working with Terraform. Try running \"terraform plan\" to see\n",
            "any changes that are required for your infrastructure. All Terraform commands\n",
            "should now work.\n",
            "\n",
            "If you ever set or change modules or backend configuration for Terraform,\n",
            "rerun this command to reinitialize your working directory. If you forget, other\n",
            "commands will detect it and remind you to do so if necessary.\u001b[0m\n",
            "\u001b[0m\u001b[1mazurerm_resource_group.rg: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_data_factory.df: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_mssql_server.sqlserver: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_storage_account.st: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1mazurerm_mssql_database.sqldb: Refreshing state... [id=/subscription=removed-for-safety\n",
            "\n",
            "Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n",
            "  \u001b[32m+\u001b[0m create\u001b[0m\n",
            "\n",
            "Terraform will perform the following actions:\n",
            "\n",
            "\u001b[1m  # azurerm_databricks_workspace.dbw\u001b[0m will be created\n",
            "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"removed-for-safety\" \"dbw\" {\n",
            "      \u001b[32m+\u001b[0m\u001b[0m customer_managed_key_enabled      = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m disk_encryption_set_id            = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m id                                = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m removed-for-safety = false\n",
            "      \u001b[32m+\u001b[0m\u001b[0m location                          = \"eastus\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m managed_disk_identity             = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m managed_resource_group_id         = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m managed_resource_group_name       = \"removed-for-safety\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m name                              = \"dbw-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m public_network_access_enabled     = true\n",
            "      \u001b[32m+\u001b[0m\u001b[0m resource_group_name               = \"rg-pipeline-demo\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m sku                               = \"standard\"\n",
            "      \u001b[32m+\u001b[0m\u001b[0m storage_account_identity          = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m workspace_id                      = (known after apply)\n",
            "      \u001b[32m+\u001b[0m\u001b[0m workspace_url                     = (known after apply)\n",
            "\n",
            "      \u001b[32m+\u001b[0m\u001b[0m custom_parameters (known after apply)\n",
            "    }\n",
            "\n",
            "\u001b[1mPlan:\u001b[0m 1 to add, 0 to change, 0 to destroy.\n",
            "\u001b[0m\n",
            "\n",
            "\n",
            "Saved the plan to: tfplan\n",
            "\n",
            "To perform exactly these actions, run the following command to apply:\n",
            "    terraform apply \"tfplan\"\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Creating...\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [1m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [1m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [1m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [1m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [1m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [1m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [2m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [2m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [2m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [2m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [2m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [2m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [3m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [3m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [3m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [3m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [3m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [3m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [4m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [4m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [4m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [4m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [4m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [4m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [5m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [5m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [5m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [5m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [5m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [5m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [6m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [6m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [6m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [6m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [6m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [6m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [7m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [7m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [7m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [7m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [7m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [7m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [8m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [8m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [8m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [8m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [8m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [8m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [9m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [9m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [9m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [9m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [9m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [9m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [10m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [10m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [10m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [10m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [10m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [10m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [11m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [11m10s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [11m20s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [11m30s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [11m40s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [11m50s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Still creating... [12m0s elapsed]\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[removed-for-safety.dbw: Creation complete after 12m9s [id=/subscription=removed-for-safety\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n",
            "\u001b[0m\u001b[0m\u001b[1m\u001b[32m\n",
            "Outputs:\n",
            "\n",
            "\u001b[0mdata_factory_name = \"dfpipeline123\"\n",
            "resource_group_name = \"rg-pipeline-demo\"\n",
            "sql_database_name = \"sqldbpipeline\"\n",
            "storage_account_name = \"storagepipeline123\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuevamente se genera listado de recursos\n",
        "\n",
        "!az resource list --resource-group rg-pipeline-demo -o table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFdYc8dqIw1Q",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758556540696,
          "user_tz": 180,
          "elapsed": 1752,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "7728ecb2-be0b-4c89-f0c6-72568fa73503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name                                       ResourceGroup     Location    Type                               Status\n",
            "removed-for-safety  ----------------  ----------  removed-for-safety  --------\n",
            "dfpipeline123                              rg-pipeline-demo  eastus      Microsoft.DataFactory/factories\n",
            "storagepipeline123                         rg-pipeline-demo  eastus      Microsoft.Storage/storageAccounts\n",
            "sqlserverpipeline123eastus2                rg-pipeline-demo  eastus2     Microsoft.Sql/servers\n",
            "sqlserverpipeline123eastus2/sqldbpipeline  rg-pipeline-demo  eastus2     Microsoft.Sql/servers/databases\n",
            "sqlserverpipeline123eastus2/master         rg-pipeline-demo  eastus2     Microsoft.Sql/servers/databases\n",
            "dbw-pipeline-demo                          rg-pipeline-demo  eastus      Microsoft.Databricks/workspaces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Con la infra estructura ya creada, se procederá a realizar el pipeline usando los servicios\n"
      ],
      "metadata": {
        "id": "6lFgG3DoO97f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear contenedor llamado 'datos' en blob storage\n",
        "!az storage container create \\\n",
        "  --name datos \\\n",
        "  --account-name storagepipeline123 \\\n",
        "  --auth-mode login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg35kNQgRfXz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758558889991,
          "user_tz": 180,
          "elapsed": 2925,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "6ae8c377-aefd-4dfc-aea6-ee4c961fc081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"created\": true\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir archivo ventas.csv al contenedor 'datos'\n",
        "!az storage blob upload \\\n",
        "  --account-name storagepipeline123 \\\n",
        "  --container-name datos \\\n",
        "  --name ventas.csv \\\n",
        "  --file \"/content/drive/MyDrive/Proyecto Azure/Datos csv/ventas.csv\" \\\n",
        "  --auth-mode key \\\n",
        "  --account-key \"bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/Znp0o5M9bab4n6p37lIzdiagv4Kh3Oxh+AStOPvhow==\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIdCutv7R2bq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758559513724,
          "user_tz": 180,
          "elapsed": 1674,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "d0472766-4f42-4e8d-83b7-58890cda5f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rAlive[################################################################]  100.0000%\rFinished[#############################################################]  100.0000%\n",
            "{\n",
            "  \"removed-for-safety\": \"removed-for-safety\",\n",
            "  \"content_md5\": \"QqhTLTjeAnwf6VwBr7XVag==\",\n",
            "  \"date\": \"removed-for-safety\",\n",
            "  \"encryption_key_sha256\": null,\n",
            "  \"encryption_scope\": null,\n",
            "  \"etag\": \"\\\"0x8DDF9EED5453DEE\\\"\",\n",
            "  \"lastModified\": \"removed-for-safety\",\n",
            "  \"request_id\": \"removed-for-safety\",\n",
            "  \"request_server_encrypted\": true,\n",
            "  \"version\": \"2022-11-02\",\n",
            "  \"version_id\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar blobs en el contenedor 'datos'\n",
        "!az storage blob list \\\n",
        "  --account-name storagepipeline123 \\\n",
        "  --container-name datos \\\n",
        "  --auth-mode key \\\n",
        "  --account-key \"bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/Znp0o5M9bab4n6p37lIzdiagv4Kh3Oxh+AStOPvhow==\" \\\n",
        "  --output table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSK_Lt29Uibu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758559632966,
          "user_tz": 180,
          "elapsed": 1618,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "ea4627a9-8bb2-4794-a956-fb6e4e1fb178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name        Blob Type    Blob Tier    Length    Content Type    Last Modified              Snapshot\n",
            "----------  -----------  -----------  --------  --------------  removed-for-safety  ----------\n",
            "ventas.csv  BlockBlob    Hot          582       text/csv        2025-09-22T15:43:53+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar que el código pyspark fue subido a mydrive\n",
        "!ls \"/content/drive/MyDrive/Proyecto Azure/Códigos/pipeline_ventas.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEFRrYDyTaEF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758561701361,
          "user_tz": 180,
          "elapsed": 356,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "e908d13a-84e1-4ee7-8bd3-3b31afc186d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'removed-for-safety'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener token dinámico de Azure AD para Databricks. Dura 1 hora aprox.\n",
        "!az account get-access-token \\\n",
        "  --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \\\n",
        "  --query accessToken -o tsv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxizm3X-clea",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758563461693,
          "user_tz": 180,
          "elapsed": 2124,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "1a8f22c8-6198-4573-a73f-9566b5eae70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed-for-safety.removed-for-safety.removed-for-safety\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Databricks CLI en Colab\n",
        "!pip install databricks-cli\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP3bauSokimd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758563820864,
          "user_tz": 180,
          "elapsed": 10420,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "b44be622-8530-43fe-dd44-b76393c03c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting databricks-cli\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (8.2.1)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.32.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (2025.8.3)\n",
            "Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: databricks-cli\n",
            "Successfully installed databricks-cli-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar token dinámico en variable de entorno\n",
        "%env DATABRICKS_AAD_TOKEN=\n",
        "\"removed-for-safety\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "TAm9U1HGj4-I",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758564777152,
          "user_tz": 180,
          "elapsed": 101,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "d0429768-d297-4b9e-df0d-5c5c3228105e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: DATABRICKS_AAD_TOKEN=removed-for-safety\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'removed-for-safety'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar Databricks CLI con token dinámico\n",
        "!databricks configure --aad-token --host https://adb-1085091664027594.14.azuredatabricks.net\n"
      ],
      "metadata": {
        "id": "nIBCVkR5qxnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Guardar host y token en variables de entorno\n",
        "os.environ[\"DATABRICKS_HOST\"] = \"https://adb-1085091664027594.14.azuredatabricks.net\"\n",
        "os.environ[\"removed-for-safety\"\n",
        "\n"
      ],
      "metadata": {
        "id": "mOg14SG7rLSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar carpetas raíz del workspace\n",
        "!databricks workspace ls /\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_Kst6bZpuSC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758565970517,
          "user_tz": 180,
          "elapsed": 3008,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "c4c3977d-9280-4c79-d9fd-5920610a1828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36mUsers\u001b[0m\n",
            "\u001b[36mShared\u001b[0m\n",
            "\u001b[36mRepos\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir el script pipeline_ventas.py al workspace de Databricks en la carpeta /Shared\n",
        "!databricks workspace import \\\n",
        "    \"/content/drive/MyDrive/Proyecto Azure/Códigos/pipeline_ventas.py\" \\\n",
        "    \"/Shared/pipeline_ventas\" \\\n",
        "    --language PYTHON \\\n",
        "    --format SOURCE\n"
      ],
      "metadata": {
        "id": "15oRMT_Gwb8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!databricks workspace ls /Shared\n"
      ],
      "metadata": {
        "id": "W-yJIfumxKEl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758567122324,
          "user_tz": 180,
          "elapsed": 1718,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "5c0ff831-ddec-4621-801b-ecd8355c520e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pipeline_ventas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Una vez creada la infraestructura usando terraform. Al voler a conectar a Colab solo es necesario realizar la siguiente configuración inicial.\n",
        "\n",
        "# Instalar Azure CLI\n",
        "!curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
        "\n",
        "# Instalar Databricks CLI\n",
        "!pip install databricks-cli\n"
      ],
      "metadata": {
        "id": "dKJH28E2xKJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758632625541,
          "user_tz": 180,
          "elapsed": 29175,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "4c3332ca-a8b9-4500-d694-b22d943f8ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.225.47.86)] [Con\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.225.47.86)] [Con\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 https://packages.microsoft.com/repos/azure-cli jammy InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,917 B in 1s (2,677 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'removed-for-safety' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lsb-release is already the newest version (11.1.0ubuntu4).\n",
            "ca-certificates is already the newest version (20240203~22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.4).\n",
            "apt-transport-https is already the newest version (2.4.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Types: deb\n",
            "URIs: https://packages.microsoft.com/repos/azure-cli/\n",
            "Suites: jammy\n",
            "Components: main\n",
            "Architectures: amd64\n",
            "Signed-by: /etc/apt/keyrings/microsoft.gpg\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://packages.microsoft.com/repos/azure-cli jammy InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,917 B in 2s (1,840 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'removed-for-safety' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "azure-cli is already the newest version (2.77.0-1~jammy).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Collecting databricks-cli\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (8.2.1)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.32.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (2025.8.3)\n",
            "Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: databricks-cli\n",
            "Successfully installed databricks-cli-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "HJyX1DBDxKNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758632763065,
          "user_tz": 180,
          "elapsed": 87842,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "b2aa73bb-07f7-46ed-d3eb-a00c4580ae3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar sesión interactiva en Azure\n",
        "!az login --use-device-code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb1UyFV5spCD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758633144993,
          "user_tz": 180,
          "elapsed": 100224,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "6b29dec2-65b6-493b-c3a9-20f544e9913c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code E5K45GSAP to authenticate.\u001b[0m\n",
            "\n",
            "Retrieving tenant=removed-for-safety and subscription=removed-for-safety for the selection...\n",
            "\n",
            "[Tenant=removed-for-safety and subscription=removed-for-safety selection]\n",
            "\n",
            "No     Subscription=removed-for-safety name     Subscription=removed-for-safety ID                       Tenant=removed-for-safety\n",
            "-----  removed-for-safety  removed-for-safety  removed-for-safety\n",
            "\u001b[96m[1]\u001b[0m *  \u001b[96mAzure subscription=removed-for-safety 1\u001b[0m  \u001b[96mremoved-for-safety\u001b[0m  \u001b[96mDirectorio predeterminado\u001b[0m\n",
            "\n",
            "The default is marked with an *; the default tenant=removed-for-safety is 'removed-for-safety' and subscription=removed-for-safety is 'removed-for-safety' (removed-for-safety).\n",
            "\n",
            "Select a subscription=removed-for-safety and tenant=removed-for-safety (Type a number or Enter for no changes): 1\n",
            "\n",
            "Tenant=removed-for-safety Directorio predeterminado\n",
            "Subscription=removed-for-safety Azure subscription=removed-for-safety 1 (removed-for-safety)\n",
            "\n",
            "[Announcements]\n",
            "With the new Azure CLI login experience, you can select the subscription=removed-for-safety you want to use more easily. Learn more about it and its configuration at https://go.microsoft.com/fwlink/?linkid=2271236\n",
            "\n",
            "If you encounter any problem, please open an issue at https://aka.ms/azclibug\n",
            "\n",
            "\u001b[93m[Warning] The login output has been updated. Please be aware that it no longer displays the full list of available subscription=removed-for-safety by default.\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener token dinámico de Azure AD\n",
        "!az account get-access-token \\\n",
        "  --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \\\n",
        "  --query accessToken -o tsv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r01qQfCsSe8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758638564043,
          "user_tz": 180,
          "elapsed": 1565,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "b65cec7d-b260-4e42-ca06-097cb1103697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed-for-safety.removed-for-safety.removed-for-safety\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegar el valor del token dinámico en DATABRICKS_TOKEN\n",
        "\n",
        "import os\n",
        "os.environ[\"DATABRICKS_HOST\"] = \"https://adb-1085091664027594.14.azuredatabricks.net\"\n",
        "os.environ[\"removed-for-safety\""
      ],
      "metadata": {
        "id": "3Z4iztPttGUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir el archivo pipeline_ventas.py al workspace de Databricks en la carpeta /Shared\n",
        "!databricks workspace import /content/drive/MyDrive/Proyecto\\ Azure/Códigos/pipeline_ventas.py /Shared/pipeline_ventas.py -l PYTHON\n",
        "\n",
        "# Listar los archivos en la carpeta /Shared para verificar que el archivo se subió correctamente\n",
        "!databricks workspace ls /Shared\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LeFsql_xXm-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758634300741,
          "user_tz": 180,
          "elapsed": 7032,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "e03e7e7b-d7e3-4623-a90e-475e41fe8b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pipeline_ventas\n",
            "pipeline_ventas.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actualizar a Jobs API 2.1\n",
        "!databricks jobs configure --version=2.1\n"
      ],
      "metadata": {
        "id": "ZCR8I-S65BRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar token nuevo en variable de entorno\n",
        "%env DATABRICKS_AAD_TOKEN=\"removed-for-safety\"\n",
        "\n",
        "# Configurar CLI para usar autenticación con token AAD\n",
        "!databricks configure --aad-token --host https://adb-1085091664027594.14.azuredatabricks.net\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vU0CklEDNIV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758638976698,
          "user_tz": 180,
          "elapsed": 905,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "02e13a1e-d1be-4a3e-bccf-346c66cc4d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: DATABRICKS_AAD_TOKEN=removed-for-safety\"removed-for-safety\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!databricks jobs create --json '{\"removed-for-safety\":\"Standard_D3_v2\",\"num_workers\":1}}]}'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eicLlsC0DXph",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758639002453,
          "user_tz": 180,
          "elapsed": 2259,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "35551bdd-37da-4c6d-f8ba-5db2b8e92c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARN\u001b[0m: Your CLI is configured to use Jobs API 2.0. In order to use the latest Jobs features please upgrade to 2.1: 'removed-for-safety'. Future versions of this CLI will default to the new Jobs API. Learn more at https://docs.databricks.com/dev-tools/cli/jobs-cli.html\n",
            "{\n",
            "  \"job_id\": 203102795079069\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar el job existente usando el job_id entregado\n",
        "!databricks jobs run-now --job-id 203102795079069"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQRUyWIEEKeD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758639246038,
          "user_tz": 180,
          "elapsed": 1727,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "38e85d4e-596a-4418-cbd5-7a4ee05ba13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARN\u001b[0m: Your CLI is configured to use Jobs API 2.0. In order to use the latest Jobs features please upgrade to 2.1: 'removed-for-safety'. Future versions of this CLI will default to the new Jobs API. Learn more at https://docs.databricks.com/dev-tools/cli/jobs-cli.html\n",
            "{\n",
            "  \"run_id\": 288004307765517,\n",
            "  \"number_in_job\": 288004307765517\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar estado del run actual\n",
        "!databricks runs get --run-id 288004307765517\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt7GU7NTEqnQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758639958087,
          "user_tz": 180,
          "elapsed": 1527,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "37bf7475-4c49-4040-82b6-26bd8bb8b8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARN\u001b[0m: Your CLI is configured to use Jobs API 2.0. In order to use the latest Jobs features please upgrade to 2.1: 'removed-for-safety'. Future versions of this CLI will default to the new Jobs API. Learn more at https://docs.databricks.com/dev-tools/cli/jobs-cli.html\n",
            "{\n",
            "  \"job_id\": 203102795079069,\n",
            "  \"run_id\": 288004307765517,\n",
            "  \"creator_user_name\": \"removed-for-safety\",\n",
            "  \"number_in_job\": 288004307765517,\n",
            "  \"original_attempt_run_id\": 288004307765517,\n",
            "  \"state\": {\n",
            "    \"life_cycle_state\": \"INTERNAL_ERROR\",\n",
            "    \"result_state\": \"FAILED\",\n",
            "    \"state_message\": \"removed-for-safety'0923-135248-7js5eyvv'removed-for-safety'Following SKUs have failed for Capacity Restrictions: Standard_D3_v2'removed-for-safety'eastus'removed-for-safety'Following SKUs have failed for Capacity Restrictions: Standard_D3_v2'removed-for-safety'eastus'removed-for-safety\",\n",
            "    \"removed-for-safety\": false\n",
            "  },\n",
            "  \"task\": {\n",
            "    \"spark_python_task\": {\n",
            "      \"python_file\": \"removed-for-safety\"\n",
            "    }\n",
            "  },\n",
            "  \"cluster_spec\": {\n",
            "    \"new_cluster\": {\n",
            "      \"spark_version\": \"13.3.x-scala2.12\",\n",
            "      \"azure_attributes\": {\n",
            "        \"availability\": \"ON_DEMAND_AZURE\"\n",
            "      },\n",
            "      \"node_type_id\": \"Standard_D3_v2\",\n",
            "      \"enable_elastic_disk\": true,\n",
            "      \"num_workers\": 1\n",
            "    }\n",
            "  },\n",
            "  \"cluster_instance\": {\n",
            "    \"cluster_id\": \"0923-135248-7js5eyvv\"\n",
            "  },\n",
            "  \"start_time\": 1758635565491,\n",
            "  \"setup_duration\": 0,\n",
            "  \"execution_duration\": 0,\n",
            "  \"cleanup_duration\": 635000,\n",
            "  \"end_time\": 1758636201074,\n",
            "  \"run_duration\": 635583,\n",
            "  \"trigger\": \"ONE_TIME\",\n",
            "  \"run_name\": \"job_pipeline_ventas_d3\",\n",
            "  \"run_page_url\": \"removed-for-safety\",\n",
            "  \"run_type\": \"JOB_RUN\",\n",
            "  \"attempt_number\": 0,\n",
            "  \"format\": \"SINGLE_TASK\",\n",
            "  \"status\": {\n",
            "    \"state\": \"TERMINATED\",\n",
            "    \"termination_details\": {\n",
            "      \"code\": \"CLUSTER_ERROR\",\n",
            "      \"type\": \"CLOUD_FAILURE\",\n",
            "      \"message\": \"removed-for-safety'0923-135248-7js5eyvv'removed-for-safety'Following SKUs have failed for Capacity Restrictions: Standard_D3_v2'removed-for-safety'eastus'removed-for-safety'Following SKUs have failed for Capacity Restrictions: Standard_D3_v2'removed-for-safety'eastus'removed-for-safety\"\n",
            "    }\n",
            "  },\n",
            "  \"job_run_id\": 288004307765517\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se hicieron varios intentos con DataBricks pero hubo problema relacionado con capacidad. Por lo tanto, se va a proceder a hacer el cambio directo con pySpark."
      ],
      "metadata": {
        "id": "ONBuYwoa6_jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar PySpark\n",
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HagKic7T4FQx",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758652827536,
          "user_tz": 180,
          "elapsed": 9388,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "9a668110-9d7f-491f-cdd2-a18a9ebbce6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"pipeline_ventas\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Confirmar versión y sesión\n",
        "print(\"✅ SparkSession creada con versión:\", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tHtTZmm4jU3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758652954407,
          "user_tz": 180,
          "elapsed": 11278,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "be49e313-8107-455f-a49f-d3afff892e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SparkSession creada con versión: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar conectores de Hadoop y Azure\n",
        "!wget -nc https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.4/hadoop-azure-3.3.4.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbcvuzLWDglT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758655814196,
          "user_tz": 180,
          "elapsed": 344,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "fce1eea2-470c-41a7-bc1a-5141e2833384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-23 18:28:53--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.4/hadoop-azure-3.3.4.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 574116 (561K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/hadoop-azure-3.3.4.jar’\n",
            "\n",
            "\rhadoop-azure-3.3.4.   0%[                    ]       0  --.-KB/s               \rhadoop-azure-3.3.4. 100%[===================>] 560.66K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-23 18:28:53 (23.6 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/hadoop-azure-3.3.4.jar’ saved [574116/574116]\n",
            "\n",
            "--2025-09-23 18:28:53--  https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 911097 (890K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/azure-storage-8.6.6.jar’\n",
            "\n",
            "azure-storage-8.6.6 100%[===================>] 889.74K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-23 18:28:53 (24.0 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/azure-storage-8.6.6.jar’ saved [911097/911097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar librerías necesarias para Blob Storage\n",
        "!wget -nc https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.4/hadoop-azure-3.3.4.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g2TDkNZTdxc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758660005903,
          "user_tz": 180,
          "elapsed": 341,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "c0567b9e-9906-4cc8-edd6-3f59a153f51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-23 19:38:45--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.4/hadoop-azure-3.3.4.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 574116 (561K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/hadoop-azure-3.3.4.jar’\n",
            "\n",
            "\rhadoop-azure-3.3.4.   0%[                    ]       0  --.-KB/s               \rhadoop-azure-3.3.4. 100%[===================>] 560.66K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-23 19:38:45 (34.7 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/hadoop-azure-3.3.4.jar’ saved [574116/574116]\n",
            "\n",
            "--2025-09-23 19:38:45--  https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 911097 (890K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/azure-storage-8.6.6.jar’\n",
            "\n",
            "azure-storage-8.6.6 100%[===================>] 889.74K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-23 19:38:45 (41.7 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/azure-storage-8.6.6.jar’ saved [911097/911097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar jars requeridos\n",
        "!wget -nc https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.4/hadoop-azure-3.3.4.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.51.v20230217/jetty-util-9.4.51.v20230217.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyIO0ALgUQ15",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758660208265,
          "user_tz": 180,
          "elapsed": 357,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "937639c1-b297-4b44-ddd3-90082b3fb06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/hadoop-azure-3.3.4.jar’ already there; not retrieving.\n",
            "\n",
            "File ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/azure-storage-8.6.6.jar’ already there; not retrieving.\n",
            "\n",
            "--2025-09-23 19:42:07--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.51.v20230217/jetty-util-9.4.51.v20230217.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 583590 (570K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-9.4.51.v20230217.jar’\n",
            "\n",
            "jetty-util-9.4.51.v 100%[===================>] 569.91K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-23 19:42:07 (30.7 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-9.4.51.v20230217.jar’ saved [583590/583590]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jetty util (para JSON Convertor)\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.51.v20230217/jetty-util-9.4.51.v20230217.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "\n",
        "# Jetty client (dependencia indirecta de hadoop-azure)\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-client/9.4.51.v20230217/jetty-client-9.4.51.v20230217.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "\n",
        "# Jetty io (también requerido por util)\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.51.v20230217/jetty-io-9.4.51.v20230217.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6xrnpsFUyfK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758660343577,
          "user_tz": 180,
          "elapsed": 358,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "4a2e3e5c-1c4d-461a-d9b1-8b42ddb00792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-9.4.51.v20230217.jar’ already there; not retrieving.\n",
            "\n",
            "--2025-09-23 19:44:23--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-client=removed-for-safety\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 327919 (320K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-client=removed-for-safety\n",
            "\n",
            "jetty-client=removed-for-safety 100%[===================>] 320.23K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-09-23 19:44:23 (21.6 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-client=removed-for-safety saved [327919/327919]\n",
            "\n",
            "--2025-09-23 19:44:23--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.51.v20230217/jetty-io-9.4.51.v20230217.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 183020 (179K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-io-9.4.51.v20230217.jar’\n",
            "\n",
            "jetty-io-9.4.51.v20 100%[===================>] 178.73K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-09-23 19:44:23 (16.6 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-io-9.4.51.v20230217.jar’ saved [183020/183020]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar Jetty 8.x jars que contienen JSON$Convertor\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty/8.1.14.v20131031/jetty-8.1.14.v20131031.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/8.1.14.v20131031/jetty-io-8.1.14.v20131031.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj5ilB2VVfD3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758660528785,
          "user_tz": 180,
          "elapsed": 516,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "21eb037f-d602-40b5-dd1b-d40b27080ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-23 19:47:28--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 287680 (281K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-8.1.14.v20131031.jar’\n",
            "\n",
            "\r          jetty-uti   0%[                    ]       0  --.-KB/s               \rjetty-util-8.1.14.v 100%[===================>] 280.94K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-23 19:47:28 (17.6 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-8.1.14.v20131031.jar’ saved [287680/287680]\n",
            "\n",
            "--2025-09-23 19:47:28--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty/8.1.14.v20131031/jetty-8.1.14.v20131031.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-09-23 19:47:28 ERROR 404: Not Found.\n",
            "\n",
            "--2025-09-23 19:47:28--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/8.1.14.v20131031/jetty-io-8.1.14.v20131031.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104219 (102K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-io-8.1.14.v20131031.jar’\n",
            "\n",
            "jetty-io-8.1.14.v20 100%[===================>] 101.78K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2025-09-23 19:47:28 (17.2 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-io-8.1.14.v20131031.jar’ saved [104219/104219]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: borrar jars modernos de Jetty que entran en conflicto\n",
        "!rm -f /usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-*.jar\n",
        "!rm -f /usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-*.jar\n",
        "!rm -f /usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-io-*.jar\n",
        "\n",
        "# Paso 2: descargar las versiones viejas de Jetty (8.1.14) que sí traen JSON$Convertor\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty/8.1.14.v20131031/jetty-8.1.14.v20131031.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/8.1.14.v20131031/jetty-io-8.1.14.v20131031.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "\n",
        "# Paso 3: reiniciar SparkSession con los jars corregidos\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"PipelineVentas\")\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/python3.12/dist-packages/pyspark/jars/*\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Paso 4: configurar credenciales de Azure Blob Storage\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.storagepipeline123.blob.core.windows.net\",\n",
        "    \"bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/Znp0o5M9bab4n6p37lIzdiagv4Kh3Oxh+AStOPvhow==\"\n",
        ")\n",
        "\n",
        "# Paso 5: definir ruta del blob\n",
        "ruta_blob = \"wasbs://datos@storagepipeline123.blob.core.windows.net/ventas.csv\"\n",
        "\n",
        "# Paso 6: leer CSV y mostrar resultados\n",
        "df = spark.read.option(\"header\", True).csv(ruta_blob)\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xfjjKGeLWmCI",
        "executionInfo": {
          "status": "error",
          "timestamp": 1758818989358,
          "user_tz": 180,
          "elapsed": 1211,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "b4e590ae-1186-49c5-f139-20b372e013c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-25 15:48:27--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 287680 (281K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-8.1.14.v20131031.jar’\n",
            "\n",
            "\r          jetty-uti   0%[                    ]       0  --.-KB/s               \rjetty-util-8.1.14.v 100%[===================>] 280.94K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-09-25 15:48:27 (8.85 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-util-8.1.14.v20131031.jar’ saved [287680/287680]\n",
            "\n",
            "--2025-09-25 15:48:27--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty/8.1.14.v20131031/jetty-8.1.14.v20131031.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-09-25 15:48:28 ERROR 404: Not Found.\n",
            "\n",
            "--2025-09-25 15:48:28--  https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/8.1.14.v20131031/jetty-io-8.1.14.v20131031.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104219 (102K) [application/java-archive]\n",
            "Saving to: ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-io-8.1.14.v20131031.jar’\n",
            "\n",
            "jetty-io-8.1.14.v20 100%[===================>] 101.78K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-09-25 15:48:28 (6.66 MB/s) - ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/jetty-io-8.1.14.v20131031.jar’ saved [104219/104219]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o35.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-399133302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Paso 6: leer CSV y mostrar resultados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Reiniciar Spark con los jars agregados\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"PipelineVentas\")\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/python3.12/dist-packages/pyspark/jars/*\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Configurar credenciales de Azure Blob\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.storagepipeline123.blob.core.windows.net\",\n",
        "    \"bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/Znp0o5M9bab4n6p37lIzdiagv4Kh3Oxh+AStOPvhow==\"\n",
        ")\n",
        "\n",
        "# Definir ruta\n",
        "ruta_blob = \"wasbs://datos@storagepipeline123.blob.core.windows.net/ventas.csv\"\n",
        "\n",
        "# Leer CSV\n",
        "df = spark.read.option(\"header\", True).csv(ruta_blob)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "vyXSKR6wB5R1",
        "executionInfo": {
          "status": "error",
          "timestamp": 1758660583745,
          "user_tz": 180,
          "elapsed": 182,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "17249129-b50f-4f2f-aaa5-1c537e1e0ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o72.csv.\n: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/ajax/JSON$Convertor\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.createDefaultStore(NativeAzureFileSystem.java:1485)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.initialize(NativeAzureFileSystem.java:1410)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.ajax.JSON$Convertor\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\t... 27 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4186128580.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Leer CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o72.csv.\n: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/ajax/JSON$Convertor\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.createDefaultStore(NativeAzureFileSystem.java:1485)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.initialize(NativeAzureFileSystem.java:1410)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.ajax.JSON$Convertor\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\t... 27 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Paso 1: crear SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"PipelineVentas\")\n",
        "    .config(\"spark.jars.packages\",\n",
        "            \"org.apache.hadoop:hadoop-azure:3.3.4,org.apache.hadoop:hadoop-azure-datalake:3.3.4\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Paso 2: configurar la clave de la cuenta\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.storagepipeline123.dfs.core.windows.net\",\n",
        "    \"bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/Znp0o5M9bab4n6p37lIzdiagv4Kh3Oxh+AStOPvhow==\"\n",
        ")\n",
        "\n",
        "# Paso 3: ruta en formato abfss:// (nuevo driver ABFS)\n",
        "ruta_blob = \"abfss://datos@storagepipeline123.dfs.core.windows.net/ventas.csv\"\n",
        "\n",
        "# Paso 4: leer CSV\n",
        "df = spark.read.option(\"header\", True).csv(ruta_blob)\n",
        "\n",
        "# Paso 5: mostrar resultados\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J50Ya7-1YLwP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758661248753,
          "user_tz": 180,
          "elapsed": 14921,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "c49cd78e-948f-4751-97d0-c13ed90b7d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+--------+------+\n",
            "| id|     fecha| producto|cantidad|precio|\n",
            "+---+----------+---------+--------+------+\n",
            "|  1|2025-09-01|  manzana|      10|   500|\n",
            "|  2|2025-09-01|     pera|       5|   700|\n",
            "|  3|2025-09-02|      uva|       8|  1000|\n",
            "|  4|2025-09-02|  plátano|      12|   300|\n",
            "|  5|2025-09-03|     kiwi|       6|   900|\n",
            "|  6|2025-09-03| frutilla|      15|   400|\n",
            "|  7|2025-09-04|  naranja|      20|   350|\n",
            "|  8|2025-09-04|mandarina|       9|   450|\n",
            "|  9|2025-09-05|     piña|       3|  1200|\n",
            "| 10|2025-09-05|    mango|       4|  1100|\n",
            "| 11|2025-09-06|   sandía|       2|  2500|\n",
            "| 12|2025-09-06|    melón|       3|  2000|\n",
            "| 13|2025-09-07|    limón|      25|   200|\n",
            "| 14|2025-09-07|   cereza|       7|  1500|\n",
            "| 15|2025-09-08| arándano|      12|   800|\n",
            "| 16|2025-09-08|  granada|       5|  1300|\n",
            "| 17|2025-09-09|     higo|       9|   700|\n",
            "| 18|2025-09-09|   papaya|       6|   950|\n",
            "| 19|2025-09-10|     coco|       2|  3000|\n",
            "| 20|2025-09-10|  durazno|      10|   600|\n",
            "+---+----------+---------+--------+------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- fecha: string (nullable = true)\n",
            " |-- producto: string (nullable = true)\n",
            " |-- cantidad: string (nullable = true)\n",
            " |-- precio: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instalar Azure CLI\n",
        "!curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
        "\n",
        "# 2. Instalar Databricks CLI (lo usarás después)\n",
        "!pip install databricks-cli\n",
        "\n",
        "# 3. Montar Google Drive (si vas a usar tus CSV desde Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 4. Iniciar sesión en Azure (modo interactivo con código en navegador)\n",
        "!az login --use-device-code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ArNaJ2UuI1g",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758823910197,
          "user_tz": 180,
          "elapsed": 111030,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "5c46fc26-e051-4922-8c94-db89961c4530"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [1 InRelease 0 B/129 kB \r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [1 InRelease 63.4 kB/129\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 80.8 kB/129 kB 63%] [Waiting for headers]\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rHit:4 https://packages.microsoft.com/repos/azure-cli jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\r                                                                               \rHit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [81.0 kB]\n",
            "Err:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages\n",
            "  File has unexpected size (80403 != 80978). Mirror sync in progress? [IP: 108.157.173.89 443]\n",
            "  Hashes of expected file:\n",
            "   - Filesize:80978 [weak]\n",
            "   - SHA512:removed-for-safety\n",
            "   - SHA256:removed-for-safety\n",
            "   - MD5Sum:removed-for-safety [weak]\n",
            "  Release file created at: Thu, 25 Sep 2025 13:16:37 +0000\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,368 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,601 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,577 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,690 kB]\n",
            "Fetched 15.9 MB in 2s (6,772 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'removed-for-safety' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Failed to fetch https://cloud.r-project.org/bin/linux/ubuntu/jammy-cran40/Packages.gz  File has unexpected size (80403 != 80978). Mirror sync in progress? [IP: 108.157.173.89 443]\n",
            "   Hashes of expected file:\n",
            "    - Filesize:80978 [weak]\n",
            "    - SHA512:removed-for-safety\n",
            "    - SHA256:removed-for-safety\n",
            "    - MD5Sum:removed-for-safety [weak]\n",
            "   Release file created at: Thu, 25 Sep 2025 13:16:37 +0000\n",
            "E: Some index files failed to download. They have been ignored, or old ones used instead.\n",
            "Requirement already satisfied: databricks-cli in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (8.2.1)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.32.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.12/dist-packages (from databricks-cli) (2.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.17.3->databricks-cli) (2025.8.3)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[93mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code EZST9WE7V to authenticate.\u001b[0m\n",
            "\n",
            "Retrieving tenant=removed-for-safety and subscription=removed-for-safety for the selection...\n",
            "\n",
            "[Tenant=removed-for-safety and subscription=removed-for-safety selection]\n",
            "\n",
            "No     Subscription=removed-for-safety name     Subscription=removed-for-safety ID                       Tenant=removed-for-safety\n",
            "-----  removed-for-safety  removed-for-safety  removed-for-safety\n",
            "\u001b[96m[1]\u001b[0m *  \u001b[96mAzure subscription=removed-for-safety 1\u001b[0m  \u001b[96mremoved-for-safety\u001b[0m  \u001b[96mDirectorio predeterminado\u001b[0m\n",
            "\n",
            "The default is marked with an *; the default tenant=removed-for-safety is 'removed-for-safety' and subscription=removed-for-safety is 'removed-for-safety' (removed-for-safety).\n",
            "\n",
            "Select a subscription=removed-for-safety and tenant=removed-for-safety (Type a number or Enter for no changes): 1\n",
            "\n",
            "Tenant=removed-for-safety Directorio predeterminado\n",
            "Subscription=removed-for-safety Azure subscription=removed-for-safety 1 (removed-for-safety)\n",
            "\n",
            "[Announcements]\n",
            "With the new Azure CLI login experience, you can select the subscription=removed-for-safety you want to use more easily. Learn more about it and its configuration at https://go.microsoft.com/fwlink/?linkid=2271236\n",
            "\n",
            "If you encounter any problem, please open an issue at https://aka.ms/azclibug\n",
            "\n",
            "\u001b[93m[Warning] The login output has been updated. Please be aware that it no longer displays the full list of available subscription=removed-for-safety by default.\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!az sql server list -o table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmNCbvfIvvfX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758818409599,
          "user_tz": 180,
          "elapsed": 2055,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "2069f3f8-a5be-4136-e239-309da1dd977d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name                         ResourceGroup     Location    AdministratorLogin\n",
            "removed-for-safety  ----------------  ----------  removed-for-safety\n",
            "sqlserverpipeline123eastus2  rg-pipeline-demo  eastus2     sqladminuser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.4/hadoop-azure-3.3.4.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n",
        "!wget -nc https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.6/azure-storage-8.6.6.jar -P /usr/local/lib/python3.12/dist-packages/pyspark/jars/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WWQiwgOFhXl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758824113935,
          "user_tz": 180,
          "elapsed": 252,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "f399aea3-6c7f-44c5-8cc3-e1d0e870decc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/hadoop-azure-3.3.4.jar’ already there; not retrieving.\n",
            "\n",
            "File ‘/usr/local/lib/python3.12/dist-packages/pyspark/jars/azure-storage-8.6.6.jar’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar bases de datos dentro del servidor SQL\n",
        "!az sql db list --server sqlserverpipeline123eastus2 --resource-group rg-pipeline-demo -o table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vF7C0PfwSfl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758818550850,
          "user_tz": 180,
          "elapsed": 1950,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "a92a12f4-a8e4-4223-c4a3-fef8a3900cba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name           Tier      Family    Capacity    MaxSize    ElasticPool\n",
            "-------------  --------  --------  ----------  ---------  -------------\n",
            "sqldbpipeline  Standard            10          250GB\n",
            "master         System                          50GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar claves del storage account\n",
        "!az storage account keys list --account-name storagepipeline123 --resource-group rg-pipeline-demo -o table\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "420ItMOnw2qR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758818698014,
          "user_tz": 180,
          "elapsed": 2242,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "e542affc-5246-4f34-e500-62667005652e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CreationTime                      KeyName    Permissions    Value\n",
            "removed-for-safety  ---------  -------------  removed-for-safety\n",
            "2025-09-22T12:53:45.516973+00:00  key1       FULL           bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/removed-for-safety+AStOPvhow==\n",
            "2025-09-22T12:53:45.516973+00:00  key2       FULL           removed-for-safety+ASt3wJk7Q==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"PipelineVentas\")\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/python3.12/dist-packages/pyspark/jars/*\")\n",
        "    .getOrCreate()\n",
        ")\n"
      ],
      "metadata": {
        "id": "SM_JYzmVFt9w",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758824173877,
          "user_tz": 180,
          "elapsed": 9073,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        }
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.storagepipeline123.blob.core.windows.net\",\n",
        "    \"bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/Znp0o5M9bab4n6p37lIzdiagv4Kh3Oxh+AStOPvhow==\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "y7QVdtnfF0x8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758824194885,
          "user_tz": 180,
          "elapsed": 21,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        }
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_blob = \"wasbs://datos@storagepipeline123.blob.core.windows.net/ventas.csv\"\n",
        "df = spark.read.option(\"header\", True).csv(ruta_blob)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "0BXB-48EGAAa",
        "executionInfo": {
          "status": "error",
          "timestamp": 1758824241264,
          "user_tz": 180,
          "elapsed": 1421,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "c4d501d0-2d57-4b2f-a9fb-989f13279146"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o30.csv.\n: java.lang.NoSuchMethodError: 'java.util.Properties org.eclipse.jetty.util.log.Log.getProperties()'\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.createPermissionJsonSerializer(AzureNativeFileSystemStore.java:429)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.<clinit>(AzureNativeFileSystemStore.java:331)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.createDefaultStore(NativeAzureFileSystem.java:1485)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.initialize(NativeAzureFileSystem.java:1410)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1138248124.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mruta_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"wasbs://datos@storagepipeline123.blob.core.windows.net/ventas.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.csv.\n: java.lang.NoSuchMethodError: 'java.util.Properties org.eclipse.jetty.util.log.Log.getProperties()'\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.createPermissionJsonSerializer(AzureNativeFileSystemStore.java:429)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.<clinit>(AzureNativeFileSystemStore.java:331)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.createDefaultStore(NativeAzureFileSystem.java:1485)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.initialize(NativeAzureFileSystem.java:1410)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear SparkSession con los paquetes correctos\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"PipelineVentas\")\n",
        "    .config(\"spark.jars.packages\",\n",
        "            \"org.apache.hadoop:hadoop-azure:3.3.4,\"\n",
        "            \"org.apache.hadoop:hadoop-azure-datalake:3.3.4\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Configurar clave de acceso\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.storagepipeline123.dfs.core.windows.net\",\n",
        "    \"bd8BA25IxGhlB+diEyhBY+tcxQ4FmOor6FVpdN6ZXEO/Znp0o5M9bab4n6p37lIzdiagv4Kh3Oxh+AStOPvhow==\"\n",
        ")\n",
        "\n",
        "# Leer CSV desde abfss://\n",
        "ruta_blob = \"abfss://datos@storagepipeline123.dfs.core.windows.net/ventas.csv\"\n",
        "df = spark.read.option(\"header\", True).csv(ruta_blob)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZYzEMASxSbZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758824548365,
          "user_tz": 180,
          "elapsed": 33001,
          "user": {
            "displayName": "Pedro",
            "userId": "10530756541937532932"
          }
        },
        "outputId": "14ceb08e-b739-498a-e745-561093066ce3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+--------+------+\n",
            "| id|     fecha| producto|cantidad|precio|\n",
            "+---+----------+---------+--------+------+\n",
            "|  1|2025-09-01|  manzana|      10|   500|\n",
            "|  2|2025-09-01|     pera|       5|   700|\n",
            "|  3|2025-09-02|      uva|       8|  1000|\n",
            "|  4|2025-09-02|  plátano|      12|   300|\n",
            "|  5|2025-09-03|     kiwi|       6|   900|\n",
            "|  6|2025-09-03| frutilla|      15|   400|\n",
            "|  7|2025-09-04|  naranja|      20|   350|\n",
            "|  8|2025-09-04|mandarina|       9|   450|\n",
            "|  9|2025-09-05|     piña|       3|  1200|\n",
            "| 10|2025-09-05|    mango|       4|  1100|\n",
            "| 11|2025-09-06|   sandía|       2|  2500|\n",
            "| 12|2025-09-06|    melón|       3|  2000|\n",
            "| 13|2025-09-07|    limón|      25|   200|\n",
            "| 14|2025-09-07|   cereza|       7|  1500|\n",
            "| 15|2025-09-08| arándano|      12|   800|\n",
            "| 16|2025-09-08|  granada|       5|  1300|\n",
            "| 17|2025-09-09|     higo|       9|   700|\n",
            "| 18|2025-09-09|   papaya|       6|   950|\n",
            "| 19|2025-09-10|     coco|       2|  3000|\n",
            "| 20|2025-09-10|  durazno|      10|   600|\n",
            "+---+----------+---------+--------+------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- fecha: string (nullable = true)\n",
            " |-- producto: string (nullable = true)\n",
            " |-- cantidad: string (nullable = true)\n",
            " |-- precio: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}